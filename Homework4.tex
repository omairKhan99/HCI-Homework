\documentclass[
	%a4paper, % Use A4 paper size
	letterpaper, % Use US letter paper size
]{jdf}

\addbibresource{references.bib}

\author{Omair Tariq Khan}
\email{okhan60@gatech.edu}
\title{Homework 4}

\begin{document}
%\lsstyle

\maketitle
\hfill \break
\hfill \break


\section{Answer to Question 1 - Conversations with ChatGPT}
Distributed cognition enables us to understand how thinking is not all in the head of an individual, but is spread across interactive system elements comprising people, artifacts and their environment. The focus moves from internal cognitive processes of an individual to explaining how acting and knowing are maintained and, in some cases, even adapted or developed within a performance system. For instance, in the article entitled How a Cockpit Remembers Its Speeds it can be seen that pilots use external representations such as speed cards and speed bugs to support memory storage/retrieval/encoding as well as decision-making. These representations act as external memory storage that reduce the burden on the working memory during demanding tasks enabling pilots to adapt rapidly instead of using cognition for remembering details (compared with slower learning or adaptation by encoding). Similar reasoning might be expected for generative AI systems like ChatGPT.

To explore these questions, I had three separate discussions with ChatGPT, each with a different goal in mind. In the first conversation, I used ChatGPT as a brainstorming partner to help me think about some ideas for a creative project. In this case, the AI system is not simply regurgitating facts from memory but actually externalizing thinking—helping to refine and build on initial ideas. The second conversation focused on using the model as an evaluator of a short piece of writing. For this role too, you can imagine that relying purely on knowledge stored in the model wouldn’t be enough. The system has to engage in judgment and analysis in order to provide helpful feedback. The chatbot acted as an external reviewer, similar to a second set of eyes on my work, but it lacked personal insight or genuine critical thinking. This interaction suggests that while AI can simulate understanding, it ultimately operates as a structured memory tool with well-organized retrieval and pattern-matching capabilities.Finally, I had a more open-ended discussion just asking ChatGPT how it could try to explain something complex at a high level so anyone could understand it.

In all of these interactions, the generative AI obviously does act as a memory. But in each case, it also seems to take on some fairly standard cognitive functions as well. In brainstorming, it acts as an external mind, allowing one’s own mind to work on less of the ideation task. In feedback, it provides something like analysis—although in ways that are similar enough to human responses that mere recall can’t be what’s going on. And in explanation, the generative AI produces understandable language for content that is already stored—as if functioning as an interpreter rather than simply a memory aid. Still, these extensions of memory-type activity can be boiled down to essentially just centralizing and improving recall functions with some more complex formatting thrown in. Even though it might seem like there is reasonableness or judgment happening at the model-level (especially when people don’t know where the model ends and humans providing output start), there is never actually any cogitation happening within this interacting cognitive system.

Distributed cognition can also provide a useful framework for considering the role of generative AI in human-computer interaction. From this perspective, a user’s interaction with an AI system can be viewed as not simply tool use, but as participation in a cognitive system more generally, in which some of the cognitive work is distributed between the human and machine. Within this view, AI has been shown to contribute to increased cognitive performance through reductions of memory load, problem-solving assistance, and communication facilitation. However, it also highlights that current examples of artificial intelligence are still fundamentally limited by their dependence on prior information and human-generated data.
\newpage

\section{Answer to Question 2}
One area where political motivations significantly shape technology design is social media content moderation, particularly regarding fact-checking and misinformation policies. I regularly encounter this issue on platforms like Facebook, Instagram, and Reddit. Meta’s recent decision to end professional fact-checking on its platforms is a prime example of how political considerations influence technological design . This move reflects a shift in how misinformation is managed, impacting what content is promoted or suppressed. Given that I use Reddit frequently, I see firsthand how political biases shape discussions, as different platforms enforce or remove content based on their moderation policies. While Reddit is often seen as a left-leaning space, its moderation strategies contrast sharply with Meta's approach, highlighting the varying political motivations behind content management.

Several key stakeholders influence this landscape. First, platform companies like Meta and Reddit have business motivations to maximize engagement and ad revenue. They must balance free speech with brand safety, as advertisers often push for stricter content controls. Second, governments and regulatory bodies exert pressure to manage misinformation, especially during election cycles or major global events. Different political groups advocate for either stricter or looser moderation, depending on their ideological stance. Lastly, users themselves are stakeholders, as they create, share, and engage with content. Some users push for open discussions, while others demand stronger interventions against false or harmful narratives. These competing motivations create tensions in content moderation policies.

The political motivations behind content moderation shape technology in multiple ways. One direct impact is the algorithmic ranking of content. Platforms prioritize certain posts based on engagement and perceived trustworthiness, affecting what information users see first. When professional fact-checking was active, posts flagged as false would be demoted, reducing their reach. With the removal of this system, misinformation may spread more easily, altering public discourse. Another way motivations influence design is through user reporting mechanisms. Some platforms rely heavily on community reports to flag misinformation, shifting the responsibility from professionals to users. This creates biases, as highly engaged groups can manipulate reporting systems to suppress opposing views. Finally, interface changes, such as warning labels or pop-up fact checks, reflect ongoing political debates. Some stakeholders advocate for strong intervention, while others argue that such labels discourage free expression. These conflicts demonstrate how political forces shape technology beyond usability, influencing how people interact with information online.

Understanding the role of political motivations in technology design helps reveal why platforms make certain moderation decisions. The tension between business interests, regulatory demands, and user preferences leads to constant changes in how content is managed. As someone who frequently navigates these platforms, I experience how different policies impact discussions and information visibility. Recognizing these influences allows for a more critical approach to engaging with online content.

\newpage

\section{Answer to Question 3}


\newpage

\section{Answer to Question 4}


\newpage

\section{References}

\printbibliography[heading=none]
\begin{enumerate}
    \item \textbf{Question 2}: \hfill \break
    Leon-Mantero C., Casas-Rosal J., Maz-Machado A., Rico M (2020, January). Analysis of attitudinal components towards statistics among students from different academic degrees
     

\end{enumerate}

\end{document}
