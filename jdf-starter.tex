\documentclass[
	%a4paper, % Use A4 paper size
	letterpaper, % Use US letter paper size
]{jdf}

\addbibresource{references.bib}

\author{Omair Tariq Khan}
\email{okhan60@gatech.edu}
\title{Homework 1}

\begin{document}
%\lsstyle

\maketitle
\hfill \break
\hfill \break


\subsection{Answer to Question 1 - Selecting and Using Apple Music}
I use Apple Music in a variety of ways, and I've noticed that each environment presents its own set of challenges, which significantly affect how I interact with the app. Whether I’m walking, sitting at home, or exercising, the user experience with the app shifts depending on my surroundings. When I’m walking down the street or navigating through a busy area, it’s harder to focus on the app because of distractions like traffic and people. It’s also tricky to tap small buttons while moving. In this context, the app could simplify the experience to reduce the extra "precision" that's needed. For example, physical buttons for things like play/pause could be introduced like the more accessible buttons for volume control that exist already. 

It would also be great if Apple integrated voice commands more prominently, allowing me to control the Music app hands-free while staying focused on the environment around me, especially in the case of non-English titles. In contrast, when I’m sitting at home or in a quiet space, I have more time and mental space to engage with Music. In this scenario, I tend to explore new albums and create playlists. The interface can afford to be more complex, letting me browse my music collection and discover new songs without any pressure or distractions. 

However, when I’m exercising, whether at the gym or outdoors the situation changes again. I’m often sweaty or wearing gloves, so this constant movement makes it hard to use small touch controls. I may also have limited hand dexterity during a workout. The action button doesn't have the capability to forward or rewind your playlist, although I think it is a handy button because it can be touched without looking at the phone itself. I also think that Apple Music could benefit from a more fitness-friendly interface. Integrating with fitness trackers could allow me to tailor the music experience based on my workout intensity, keeping me motivated and focused without needing to manually adjust the app.

To improve the experience even further, I think Apple Music could be designed to adjust dynamically depending on my context. For example, when I’m walking or driving, the app could simplify the interface and highlight larger controls. I am not looking at the album art while walking, so if there was a detection system in place, it would display the larger multimedia controls on my screen. Newer phones also have the capability of an always on screen. Multimedia buttons can also be added to that. As a user, I'm more prone to be connected to Bluetooth in my earphones or my Car or Apple CarPlay when I'm not home. To add to my point from above, Apple Music has forward, backward, and pause buttons feature on Google Maps but not Apple Maps, which does highlight its ineffectiveness. At home, the app could offer more detailed options for music exploration, such as more intricate playlist management and album browsing. 

In conclusion, using Apple Music in different situations comes with its own challenges. If it detects me walking or driving automatically, that would be great. I feel like with Siri handling basic commands like opening and closing the app itself, there just needs to be a bit more intuition on the app automatically adjusting based on these different environments. This would provide a more seamless and enjoyable experience, no matter where I am or what I’m doing. 
\newpage

\subsection{Answer to Question 2}
The process of submitting a question and receiving an answer on Ed Discussion involves several steps that highlight how the system supports and/or challenges students in achieving their goals. These steps can be analyzed using the concepts of the gulf of execution and the gulf of evaluation, focusing on how the platform helps bridge these gaps.

\subsubsection {the Gulf of Execution}
The gulf of execution begins with the student forming a goal, such as asking for clarification on a concept or receiving help with an assignment. Ed Discussion supports this planning stage by providing clear entry points for creating a new post. The “New Thread” button is prominently displayed on the interface, and categories such as “Class Discussion” or “Homework” help students know where to post. It’s also nice that you can tag your thread as either a "Post" or a "Question", each with the small icons, e.g. a "message" or "question mark", which is great. 

After deciding to ask a question, the student needs to figure out how to write it. Ed Discussion helps by giving a form with labeled boxes for the title, the question, and the category. It also has dropdown menus to make it simple to choose options. Although some students still might struggle with understanding how much detail to include or which category fits best. But, this is addressed by showing questions already asked with similar keywords that guide students in framing their questions appropriately. Making an example (rough) thread just to get an idea of the layout is good, just like we were allowed to for this Homework!

Finally, students execute their plan by filling out the form, selecting the category, and submitting their post/question. Ed Discussion provides immediate visual feedback by displaying the question on the discussion board, but it is not tagged as “Unanswered.” This does reassure students that their question has been successfully posted. However, students have to filter by "Unanswered" from the drop-down to see whether their question is answered or not. 

\subsubsection {The Gulf of Evaluation}
The gulf of evaluation begins once the question is submitted, and students need to understand the outcome of their actions. Ed Discussion helps by showing the question under the right category and has "views" and "Watching" buttons there. This visual feedback ensures that students can see their question and verify its presence on the platform. However, if the interface were less responsive or the feedback were delayed, students might worry whether their question was posted successfully.

In the interpretation stage, students must understand the status of their question and what happens next. Ed Discussion supports this by making the question visible to peers, TAs, and professors, who can then reply. The “Unanswered” tag from the drop down signals that the question is pending a response, but if there are tons of questions posted, your post can get lost. Though the filter by "Me" button does help a bit, however, some students may be unsure of the expected response time or whether their question was clear. Adding features like an estimated response time (calculating from average response time over the past few weeks, maybe?) or a confirmation that the question was received and is being reviewed could enhance this stage.

The comparison stage involves evaluating the answer after it has been received. When a TA, professor, or peer responds, Ed Discussion notifies the student and displays the response below the original question in a threaded format. This organization may or may not help students understand how the response addresses their query because lines that branch down from a response are sometimes harder to follow as to who is responding to who. If notifications are delayed or hard to see, students might miss the response. To address this, the platform does implement personalized notifications, such as email alerts.

Another way to improve the system would be to add features like small tips when hovering over buttons, or showing if someone is typing a response. Varying feedback methods, such as visual indicators showing the number of likes or people typing and auditory cues for new responses, could keep students informed and engaged. I would also like to see an option of a "personal" thread where I can post notes, or thoughts without anyone seeing them as right now, you can only hide your threads from students and not the course instructor/staff. 
\newpage

\subsection{Answer to Question 3}
\subsubsection {Activity 1: Using a Self-Checkout Machine at a Grocery Store}
Whenever I use a self-checkout machine at the grocery store, I often find it hard to figure out how to use it properly. One of the main issues is scanning produce. The machine isn’t very user-friendly, and I spend a lot of time searching through long lists of items just to find something basic like bananas. The options sometimes aren’t clear, and I feel like I’m guessing half the time. If I make a mistake, like accidentally scanning an item twice, removing it from the cart feels overly complicated because now you have to wait for someone to get to you and scan their badge. 
Sometimes, even basic tasks like paying can be confusing. I’ve had moments where the machine didn’t register that I inserted my card, leaving me staring at the screen wondering if I did something wrong or if the machine just froze. The lack of clear instructions makes the whole process stressful. 

The gulf of evaluation is also a problem with these machines. For example, when I scan an item, the machine doesn’t always respond right away, and I’ve accidentally scanned the same item twice because I wasn’t sure it went through the first time.  Then there’s the dreaded “Unexpected item in the bagging area” error. It’s such a vague message, and I’m never sure if I need to move the item, rescan it, or just wait for help. These unclear prompts make an already stressful process even more frustrating, especially when there’s a line of people waiting behind me. 

\subsubsection {Activity 2: Using a Ride-Sharing App (e.g., Uber)}
In contrast, I’ve had a much smoother experience using apps like Uber or even Lyft! These apps are simple to use, and they do a great job guiding me through the process. When I open the app, I can easily input my destination, and it even suggests common places based on my previous trips, which saves me time. After confirming my pickup location, the app keeps me informed every step of the way. I can see the driver’s exact location on the map, get an estimated time of arrival, and their live location. I travel frequently between my home state in New Jersey and Arizona (where I live), and I can set the exact terminal, door, coordinate position (north/south if your airport has those!!)to go home from the airport. When I am going to the Airport, I can book a ride well in advance and the dropoff point allows me to select the terminal and the door based on the airline I am traveling in (the door is many times not clear in your travel itinerary).

If there’s an issue, like a driver canceling, the app tells me exactly what happened and automatically starts looking for a new driver. I never feel lost or unsure of what’s going on because the app is constantly keeping me in the loop. Even the payment process is seamlessly done through ApplePay or your credit card, and I get a receipt right after the ride ends. The whole experience is smooth, clear, and stress-free. Also, while traveling you can share your live location with your friends/family just for your safety. 

Self-checkout machines could learn a lot from ride-sharing apps by improving both discoverability and feedback. For example, the interface could be made more user-friendly, with clear categories for produce and animations to show how to do things step by step. Real-time feedback, like a progress bar for scanning items or instructions that update as you go, would make it easier to understand what’s happening. A better error system, like a pop-up explaining what went wrong and how to fix it, would make the experience less frustrating and help build user confidence.
\newpage

\subsection{Answer to Question 4}
The use of tools like ChatGPT for therapy-like support raises important ethical questions. On one hand, these tools provide quick and easy help for people who can’t access traditional mental health care due to cost, stigma, or availability. So, it does seem like ChatGPT and many others like it make mental health support more accessible since we can access them on our personal devices (Farvolden, 2024). On the other hand, ChatGPT isn’t a licensed therapist. It can’t truly understand emotions, give detailed care, or handle emergencies. This could lead to people relying on it too much and not getting professional help when they need it most (Petracek, 2024).

To ethically permit such usage, OpenAI should clearly state that ChatGPT isn’t a replacement for therapy. And to some extent in many interactions, it does include disclaimers reminding users that it is not a substitute for therapy and encourages them to seek professional help. I think there is further room for improvement there. For example, if someone writes about self-harm or extreme distress, the tool should automatically direct them to a hotline or crisis resource right away. While these tools can’t replace real human care, they can still help people who might not have any other support. Even so ChatGPT can simulate empathy, it cannot replicate the nuanced human connection that is vital in therapeutic settings. Therefore, it's essential to ensure users are aware of these limitations to prevent potential harm (Coghlan et.al, 2023). 



To explore ways to enhance ChatGPT's support for mental health conversations, I suggest testing a feature that recognizes user emotions through their messages. To test this feature, an experiment would involve consenting participants using an enhanced version of ChatGPT for mental health-related conversations. Since voice input is an option, the enhanced version would only take voice input, which in my opinion will make the experience feel more personal. It would also allow the system to analyze the vocal tone to better detect the emotional state of the participant. Based on the emotion, the tool could suggest helpful exercises like mindfulness or ways to reframe negative thoughts. If the tool deems it too severe, it would immediately inform the emergency resources. For this test, we could recruit volunteers through mental health forums and social media.

The recruitment message might say: "We are studying how AI tools like ChatGPT can support mental well-being. This experimental version offers guided conversations and reflective exercises, but is not a replacement for professional care. If you’re interested, click here to learn more." Participants would sign a consent form explaining the purpose of the study, the risks, and that it is voluntary. The consent form would also outline the available crisis resources in case of any unprecedented mental episode. Now, this approach may seem transparent, it might attract people who already trust AI tools in this regard, so a possibility of a selection bias is very much there. However, I think that the experiment could give us valuable insights into how ChatGPT can safely support mental health. Ensuring responsible utilization of ChatGPT in psychological context requires a combination of caution, human judgment, and ethical regulations.


\newpage

\section{References - Question 4 only}

\printbibliography[heading=none]
\begin{enumerate}
    \item Farvolden, P. (2024). AI Chatbots Break Down Barriers to Much-Needed Mental Health Treatments. RGA Knowledge Center
    \item Petracek, L. (2024). AI Chatbots for Mental Health: Opportunities and Limitations
    \item Coghlan S., Leins, K., Sheldrick S., Cheong M., Gooding P., D'Alfonso S. (2023). To chat or bot to chat: Ethical issues with using chatbots in mental health

\end{enumerate}

\end{document}
