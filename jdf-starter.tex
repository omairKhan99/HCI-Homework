\documentclass[
	%a4paper, % Use A4 paper size
	letterpaper, % Use US letter paper size
]{jdf}

\addbibresource{references.bib}

\author{Omair Tariq Khan}
\email{okhan60@gatech.edu}
\title{Homework 1}

\begin{document}
%\lsstyle

\maketitle
\hfill \break
\hfill \break


\subsection{Answer to Question 1 - Selecting and Using Apple Music}
I listen to and use Apple Music in several environments, and I find that each one has its own series of challenges that affect how I use the app in a very meaningful way. Depending upon my environment, whether I’m walking or sitting at home or exercising, the user experience with the app changes. If I’m out walking or moving through a busy place, it’s also harder to focus on the app because there are a lot of distractions like cars and folks. Tapping small buttons while in motion also presents a challenge. In this scenario the app could streamline the experience to eliminate the additional “precision” that is required. Some of the guidelines introduced include physical buttons for play/pause, or more accessible buttons for volume control with haptic feedback, for example.

If Apple integrated voice commands to a greater degree, I could talk to my Music app while my attention is firmly embedded in the world around me too, especially in the case of non-English titles. Whereas when I’m sitting on the couch, or in a quiet room, I have more time and mental space to engage with Music. During this period I try out new albums and make playlists. The interface can be more complex, allowing me to browse my music collection and discover new songs at my own pace, without pressure and distractions.

However, when exercising, either in a gym or outside the situation is different again. I’m generally sweaty or wearing gloves, and all that constant movement makes it difficult to use small touch controls. I would also have limited hand dexterity when exercising. The action button can neither skip ahead nor rewind your playlist which is a little annoying, but I find it a handy button because you can touch the action button without looking at the phone itself. I also feel that Apple Music could use a more fitness-oriented layout. If it would integrate with fitness trackers, it could allow me to customize the music experience according to my workout, adjusting automatically to the intensity, to keep me motivated and focused without needing to touch the app while my hands are sweaty.

Apple Music could adapt itself even further, in a dynamic way based on context, and better serve my audio experience. For instance, if I’m out for a walk or driving, the app could reduce its interface complexity and bring bigger controls forward. As I am not looking at the album art as I walk, if a detection system were in place it would flash up the larger multimedia controls on my screen. Newer phones have an always on screen capability as well. You can even add multimedia buttons on top of that too. Whenever I am not home, I am more likely to be connected to Bluetooth on my earphones or my Car or Apple CarPlay. To add to my point from above, Apple Music has forward, backward, and pause buttons feature on Google Maps but not Apple Maps, which does highlight its ineffectiveness. At home, the app could provide more in-depth music discovery options, including more advanced playlist creation and album browsing.

In conclusion, Apple Music in different scenarios causes different issues. Hopefully it picks me up walking or driving automatically. With Siri now handling some of the basics like opening and closing the app itself, it just needs to be a little smarter in terms of how it can adjust based on these different environments. This would provide a more seamless and enjoyable experience, no matter where I am or what I’m doing. 
\newpage

\subsection{Answer to Question 2}
The process of submitting a question and receiving an answer on Ed Discussion involves several steps that highlight how the system supports and/or challenges students in achieving their goals. These steps can be analyzed using the concepts of the gulf of execution and the gulf of evaluation, focusing on how the platform helps bridge these gaps.

\subsubsection {the Gulf of Execution}
The gulf of execution begins with the student forming a goal, such as asking for clarification on a concept or receiving help with an assignment. Ed Discussion supports this planning stage by providing clear entry points for creating a new post. The “New Thread” button is prominently displayed on the interface, and categories such as “Class Discussion” or “Homework” help students know where to post. It’s also nice that you can tag your thread as either a "Post" or a "Question", each with the small icons, e.g. a "message" or "question mark", which is great. 

After deciding to ask a question, the student needs to figure out how to write it. Ed Discussion helps by giving a form with labeled boxes for the title, the question, and the category. It also has dropdown menus to make it simple to choose options. Although some students still might struggle with understanding how much detail to include or which category fits best. But, this is addressed by showing questions already asked with similar keywords that guide students in framing their questions appropriately. Making an example (rough) thread just to get an idea of the layout is good, just like we were allowed to for this Homework!

Finally, students execute their plan by filling out the form, selecting the category, and submitting their post/question. Ed Discussion provides immediate visual feedback by displaying the question on the discussion board, but it is not tagged as “Unanswered.” This does reassure students that their question has been successfully posted. However, students have to filter by "Unanswered" from the drop-down to see whether their question is answered or not. 

\subsubsection {The Gulf of Evaluation}
The gulf of evaluation begins once the question is submitted, and students need to understand the outcome of their actions. Ed Discussion helps by showing the question under the right category and has "views" and "Watching" buttons there. This visual feedback ensures that students can see their question and verify its presence on the platform. However, if the interface were less responsive or the feedback were delayed, students might worry whether their question was posted successfully.

In the interpretation stage, students must understand the status of their question and what happens next. Ed Discussion supports this by making the question visible to peers, TAs, and professors, who can then reply. The “Unanswered” tag from the drop down signals that the question is pending a response, but if there are tons of questions posted, your post can get lost. Though the filter by "Me" button does help a bit, however, some students may be unsure of the expected response time or whether their question was clear. Adding features like an estimated response time (calculating from average response time over the past few weeks, maybe?) or a confirmation that the question was received and is being reviewed could enhance this stage.

The comparison stage involves evaluating the answer after it has been received. When a TA, professor, or peer responds, Ed Discussion notifies the student and displays the response below the original question in a threaded format. This organization may or may not help students understand how the response addresses their query because lines that branch down from a response are sometimes harder to follow as to who is responding to who. If notifications are delayed or hard to see, students might miss the response. To address this, the platform does implement personalized notifications, such as email alerts.

Another way to improve the system would be to add features like small tips when hovering over buttons, or showing if someone is typing a response. Varying feedback methods, such as visual indicators showing the number of likes or people typing and auditory cues for new responses, could keep students informed and engaged. I would also like to see an option of a "personal" thread where I can post notes, or thoughts without anyone seeing them as right now, you can only hide your threads from students and not the course instructor/staff. 
\newpage

\subsection{Answer to Question 3}
\subsubsection {Activity 1: Using a Self-Checkout Machine at a Grocery Store}
Every time I go to a grocery store and use a self-checkout machine, it seems that I have trouble successfully using it. The biggest problem is scanning items that are weighed, like fruits and vegetables. This machine is not very user-friendly, though, and I waste so much time scrolling through long lists of products to find something as simple as bananas. The options are not always clear, and I feel like I’m guessing half the time. If I screw up, and I accidentally scan something twice, it takes too much effort to remove it from the cart, and now you have to wait for someone to come and get to you and scan their badge. Paying, even, can be confusing, at times. I’ve had instances where the machine did not realize I put my card in, me staring at the screen wondering whether I did something wrong, or the machine just froze. The absence of clear instructions makes for a stressful experience.

The gulf of evaluation is also a problem with these machines.  Like when I scan something and the machine doesn’t respond immediately, or I’ve accidentally scanned something twice because I didn’t know if it registered or not. Then there’s the dreaded “Unexpected item in bagging area” error. It’s such a nondescript message, and I never know if I need to move the item, rescan it or just stand and wait for help. Such ambiguous prompts add additional frustration on top of what is already a stressful process, especially in the case where there is a line of people waiting behind me.

\subsubsection {Activity 2: Using a Ride-Sharing App (e.g., Uber)}
By contrast, my experience using apps like Uber or even Lyft has been much smoother! These apps are easy to use and they do a good job walking me through the process. I just need to open the app, plug in the place where I am headed, and it even provides potential suggestions depending on where I have visited during previous trips, which resets the clock! Once I confirm where I want to be picked up, the app guides me along every step of the way. I can see the driver’s exact location on the map, get an estimated time of arrival, and their live location. I travel frequently between my home state in New Jersey and Arizona (where I live), and I can set the exact terminal, door, coordinate position (north/south if your airport has those!!)to go home from the airport. When I am going to the Airport, I can book a ride well in advance and the dropoff point allows me to select the terminal and the door based on the airline I am traveling in (the door is many times not clear in your travel itinerary).

If there’s an issue, like a driver canceling, the app tells me exactly what happened and automatically starts looking for a new driver. I never feel lost or unsure of what’s going on because the app is constantly keeping me in the loop. Even the payment process is seamlessly done through ApplePay or your credit card, and I get a receipt right after the ride ends. The whole experience is smooth, clear, and stress-free. Also, while traveling you can share your live location with your friends/family just for your safety. 

There are lessons here for self-checkout machines that could take a cue from ride-sharing apps and figure out how to improve both discoverability and feedback. For instance, the interface could be more user-friendly, with explicit categories for produce and Earth-friendly animations of how to-do's step-by-step. Giving real-time feedback, like a progress bar for scanning items or instructions that change as you work your way through a task, would better accommodate our lack of knowledge about what’s happening. A more informative error messaging system, such as a pop-up explaining what occurred poorly, would make the experience less frustrating and help build user confidence.
\newpage

\subsection{Answer to Question 4}
It brings up important ethical questions to use tools like ChatGPT for therapy-like support. On the one hand, these tools could offer quick, easy assistance to people who can’t get traditional mental health care because they can’t afford it, or they can’t manage the stigma, or the access conditions. Hence, it appears that ChatGPT and many similar ones like it increase the accessibility of mental health support as they are available on our personal devices (Farvolden, 2024). ChatGPT, on the other hand, is not a licensed therapist. It can’t truly understand emotions, provide nuanced care, or manage emergencies. This might cause individuals to overly depend on it, which can prevent people from receiving the professional support they might need (Petracek, 2024).

If OpenAI wants to allow such use ethically, it should be explicit that ChatGPT is not the same as therapy. And it does feature disclaimers in many interactions reminding users that it’s not a substitute for therapy and encouraging them to seek professional help, at least to some extent. I think there is further room for improvement there. For example, if someone writes about self-harm or extreme distress, the tool should automatically route them to an appropriate hotline or crisis resource at once. While these tools can’t substitute for real human care, they can still provide assistance to people who’d otherwise have none. And however well ChatGPT mimics empathy, it cannot replace the subtle human bond that is essential in therapeutic environments. So it is important to make users aware of these limitations to avoid any harm (Coghlan et.al, 2023).

To search for ideas on how ChatGPT could better facilitate mental health discussions, I propose a feature where it can detect the emotion of a user based on their messages. For this feature, an experiment could involve testing out a modified version of ChatGPT for mental health-related conversations with consenting participants. Since voice input is an option, the enhanced version would only take voice input, which I think will make the experience feel more personal. It would also allow the system to analyze the vocal tone to better detect the emotional state of the participant. Based on the emotion, the tool could suggest helpful exercises like mindfulness or ways to reframe negative thoughts. If the tool deems it too severe, it would immediately inform the emergency resources. For this test, we could recruit volunteers through mental health forums and social media.

The recruitment message might say: "We are studying how AI tools like ChatGPT can support mental well-being. This experimental version offers guided conversations and reflective exercises, but is not a replacement for professional care. If you’re interested, click here to learn more." Participants would sign a consent form explaining the purpose of the study, the risks, and that it is voluntary. The consent form would also outline the available crisis resources in case of any unprecedented mental episode. Now, this approach may seem transparent, but it might attract people who already trust AI tools in this regard, so the possibility of selection bias is very much there. However, I think that the experiment could give us valuable insights into how ChatGPT can safely support mental health. Ensuring responsible utilization of ChatGPT in a psychological context requires a combination of caution, human judgment, and ethical regulations.



\newpage

\section{References - Question 4 only}

\printbibliography[heading=none]
\begin{enumerate}
    \item Farvolden, P. (2024). AI Chatbots Break Down Barriers to Much-Needed Mental Health Treatments. RGA Knowledge Center
    \item Petracek, L. (2024). AI Chatbots for Mental Health: Opportunities and Limitations
    \item Coghlan S., Leins, K., Sheldrick S., Cheong M., Gooding P., D'Alfonso S. (2023). To chat or bot to chat: Ethical issues with using chatbots in mental health

\end{enumerate}

\end{document}
